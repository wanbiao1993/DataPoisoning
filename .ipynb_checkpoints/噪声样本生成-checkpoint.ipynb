{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81793de5-936a-45a4-a735-39cc362486a8",
   "metadata": {},
   "source": [
    "# 0. 产生噪声样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a58c0cb-21be-428f-a0cd-e2723e2f71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 基本定义.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91e348-626b-479e-adf3-e3362ebcef7d",
   "metadata": {},
   "source": [
    "# 1. 生成攻击的目标样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d036c80-624b-403e-8dd6-fc9ebd9cbcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate target triples for DATASET wn18rr\n",
      "get 100 targetTriples\n"
     ]
    }
   ],
   "source": [
    "## 修改DATASET生成不同的目标样本\n",
    "DATASET = 'wn18rr'\n",
    "if DATASET == \"FB15k-237\":\n",
    "    target_rank = 1\n",
    "elif DATASET == \"wn18rr\":\n",
    "    target_rank = 5\n",
    "else:\n",
    "    raise Exception(\"dataset should be either FB15k-237 or wn18rr\")\n",
    "data_path = f\"./data/{DATASET}\"\n",
    "print(f\"generate target triples for DATASET {DATASET}\")\n",
    "\n",
    "# load dataset info\n",
    "with open(os.path.join(data_path, 'entities.dict')) as fin:\n",
    "    entity2id = dict()\n",
    "    for line in fin:\n",
    "        eid, entity = line.strip().split('\\t')\n",
    "        entity2id[entity] = int(eid)\n",
    "\n",
    "with open(os.path.join(data_path, 'relations.dict')) as fin:\n",
    "    relation2id = dict()\n",
    "    for line in fin:\n",
    "        rid, relation = line.strip().split('\\t')\n",
    "        relation2id[relation] = int(rid)\n",
    "train_triples = read_triple(os.path.join(data_path, \"train.txt\"), entity2id, relation2id)\n",
    "valid_triples = read_triple(os.path.join(data_path, 'valid.txt'), entity2id, relation2id)\n",
    "test_triples = read_triple(os.path.join(data_path, 'test.txt'), entity2id, relation2id)\n",
    "all_true_triples = train_triples + valid_triples + test_triples\n",
    "\n",
    "entity_density = {}\n",
    "for head, relation, tail in all_true_triples:\n",
    "    if head not in entity_density:\n",
    "        entity_density[head] = 0\n",
    "    if tail not in entity_density:\n",
    "        entity_density[tail] = 0\n",
    "    entity_density[head] += 1\n",
    "    entity_density[tail] += 1\n",
    "\n",
    "# load testing results\n",
    "model2triple2rank = {}\n",
    "model2top1Triples = {}\n",
    "for MODEL in [\"TransE\", \"RotatE\"]:\n",
    "    triple2rank_path = f\"./models/{MODEL}_{DATASET}_baseline/triple2ranking.pkl\"\n",
    "    with open(triple2rank_path, \"rb\") as f:\n",
    "        triple2rank = pickle.load(f)\n",
    "    model2triple2rank[MODEL] = triple2rank\n",
    "    model2top1Triples[MODEL] = set()\n",
    "    for triple, mode2ranking in triple2rank.items():\n",
    "        rankh, rankt = mode2ranking[\"head-batch\"], mode2ranking[\"tail-batch\"]\n",
    "        if rankh <= target_rank and rankt <= target_rank:\n",
    "            model2top1Triples[MODEL].add(triple)\n",
    "\n",
    "# load \n",
    "top1Triples = model2top1Triples[\"TransE\"].intersection(model2top1Triples[\"RotatE\"])\n",
    "\n",
    "top1Triples_score = [((head, relation, tail), entity_density[head], entity_density[tail]) for head, relation, tail in top1Triples]\n",
    "top1Triples_score = sorted(top1Triples_score, key=lambda x: -(x[1] + x[2]))[:100]\n",
    "targetTriples = [triple for triple, _, _ in top1Triples_score]\n",
    "with open(os.path.join(data_path, 'targetTriples.pkl'), \"wb\") as fw:\n",
    "    print(f\"get {len(targetTriples)} targetTriples\")\n",
    "    pickle.dump(targetTriples, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7dd56db-7fa1-473e-a08f-ba639e300522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from IPython import embed\n",
    "from collections import defaultdict\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "class GlobalRandomNoiseAttacker:\n",
    "    def __init__(self, args):\n",
    "        self.name = \"GlobalRandomNoiseAttacker\"\n",
    "        self.args = args\n",
    "        self.input_data = get_input_data(args)\n",
    "        self.trainer = BaseTrainer.get_trainer(self.input_data, args)\n",
    "        self.trainer.load_model()\n",
    "        self.kge_model = self.trainer.kge_model\n",
    "        self.all_relations = list(self.input_data.relation2id.values())\n",
    "        self.all_entities = list(self.input_data.entity2id.values())\n",
    "        self.target_triples = args['target_triples']\n",
    "        if self.target_triples is None:\n",
    "            self.target_triples = pickle.load(open(os.path.join(args['data_path'], \"targetTriples.pkl\"), \"rb\"))\n",
    "        set_logger(args, args['identifier'])\n",
    "        self.noise_triples = set()\n",
    "\n",
    "    def get_noise_triples(self):\n",
    "        noise_triples = set()\n",
    "        all_true_triples = set(self.input_data.all_true_triples)\n",
    "        for i in range(len(self.target_triples)):\n",
    "            sys.stdout.write(\"%d in %d\\r\" % (i, len(self.target_triples)))\n",
    "            sys.stdout.flush()\n",
    "            # h, r, t = self.target_triples[i]\n",
    "            rand_h = random.choice(self.all_entities)\n",
    "            rand_r = random.choice(self.all_relations)\n",
    "            rand_t = random.choice(self.all_entities)\n",
    "            while (rand_h, rand_r, rand_t) in noise_triples or (rand_h, rand_r, rand_t) in all_true_triples:\n",
    "                rand_h = random.choice(self.all_entities)\n",
    "                rand_r = random.choice(self.all_relations)\n",
    "                rand_t = random.choice(self.all_entities)\n",
    "            noise_triples.add((rand_h, rand_r, rand_t))\n",
    "        return list(noise_triples)\n",
    "\n",
    "    def generate(self, identifier):\n",
    "        dataset_model = self.args['init_checkpoint'].split(\"/\")[-1]\n",
    "        print(f'------ {self.name} starts to generate noise for {dataset_model} ------')\n",
    "        start_time = time.time()\n",
    "        noise_triples = self.get_noise_triples()\n",
    "        print(f\"Time taken:{time.time() - start_time}\")\n",
    "        print(f\"Num Noise:{len(noise_triples)}\")\n",
    "        print(f\"False Negative: {len(set(noise_triples).intersection(set(self.input_data.all_true_triples)))}\")\n",
    "        if not self.args['no_store']:\n",
    "            with open(os.path.join(self.args['init_checkpoint'], \"%s.pkl\" % identifier), \"wb\") as fw:\n",
    "                pickle.dump(noise_triples, fw)\n",
    "\n",
    "\n",
    "class LocalRandomNoiseAttacker(GlobalRandomNoiseAttacker):\n",
    "    def __init__(self, args):\n",
    "        super(LocalRandomNoiseAttacker, self).__init__(args)\n",
    "        self.name = \"LocalRandomNoiseAttacker\"\n",
    "\n",
    "    def get_noise_triples(self):\n",
    "        noise_triples = set()\n",
    "        all_true_triples = set(self.input_data.all_true_triples)\n",
    "        for i in range(len(self.target_triples)):\n",
    "            sys.stdout.write(\"%d in %d\\r\" % (i, len(self.target_triples)))\n",
    "            sys.stdout.flush()\n",
    "            h, r, t = self.target_triples[i]\n",
    "            rand_r = random.choice(self.all_relations)\n",
    "            rand_e = random.choice(self.all_entities)\n",
    "            if random.random() < 0.5:\n",
    "                while (h, rand_r, rand_e) in noise_triples or (h, rand_r, rand_e) in all_true_triples:\n",
    "                    rand_r = random.choice(self.all_relations)\n",
    "                    rand_e = random.choice(self.all_entities)\n",
    "                noise_triples.add((h, rand_r, rand_e))\n",
    "            else:\n",
    "                while (rand_e, rand_r, t) in noise_triples or (rand_e, rand_r, t) in all_true_triples:\n",
    "                    rand_r = random.choice(self.all_relations)\n",
    "                    rand_e = random.choice(self.all_entities)\n",
    "                noise_triples.add((rand_e, rand_r, t))\n",
    "        return list(noise_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba5bae1-67f2-446f-81dc-c13589d2e4b8",
   "metadata": {},
   "source": [
    "# 2. 添加扰动噪声样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4003ae0a-f844-4533-a5d1-e07948a3b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectAddition(GlobalRandomNoiseAttacker):\n",
    "    def __init__(self, args):\n",
    "        super(DirectAddition, self).__init__(args)\n",
    "        self.score_func = lambda s1, s2: args['lambda1'] * s1 - args['lambda2'] * s2\n",
    "        self.name = \"direct\"\n",
    "\n",
    "        self.true_rel_head, self.true_rel_tail = defaultdict(set), defaultdict(set)\n",
    "        for triple in self.input_data.all_true_triples:\n",
    "            self.add_true_triple(triple)\n",
    "    \n",
    "    def add_true_triple(self, triple):\n",
    "        h, r, t = triple\n",
    "        self.true_rel_tail[h].add((r, t))\n",
    "        self.true_rel_head[t].add((r, h))\n",
    "\n",
    "    def get_noise_for_head(self, test_triple, mode=\"head-batch\"):\n",
    "        args = self.args\n",
    "        h, r, t = test_triple\n",
    "        true_cand = self.true_rel_tail[h] if mode == \"head-batch\" else self.true_rel_head[t]\n",
    "        s = time.time()\n",
    "        cand_r_list = random.choices(self.all_relations, k=args['num_cand'])\n",
    "        cand_e_list = random.choices(self.all_entities, k=args['num_cand'])\n",
    "        cand_r_e_list = list(set(zip(cand_r_list, cand_e_list)).difference(true_cand))\n",
    "        cand_r_list, cand_e_list = zip(*cand_r_e_list)\n",
    "        cand_r_list, cand_e_list = list(cand_r_list), list(cand_e_list)\n",
    "        args['num_cand'] = len(cand_r_list)\n",
    "\n",
    "        embed_h = self.kge_model.entity_embedding[h]\n",
    "        embed_r = self.kge_model.relation_embedding[r]\n",
    "        embed_t = self.kge_model.entity_embedding[t]\n",
    "        score = self.kge_model.score_embedding(embed_h, embed_r, embed_t)\n",
    "        perturbed_embed_h, perturbed_embed_t = None, None\n",
    "        if mode == \"head-batch\":\n",
    "            embed_h_grad = autograd.grad(score, embed_h)[0]\n",
    "            perturbed_embed_h = embed_h - args['epsilon'] * embed_h_grad\n",
    "        elif mode == \"tail-batch\":\n",
    "            embed_t_grad = autograd.grad(score, embed_t)[0]\n",
    "            perturbed_embed_t = embed_t - args['epsilon'] * embed_t_grad\n",
    "\n",
    "        b_begin = 0\n",
    "        cand_scores = []\n",
    "        with torch.no_grad():\n",
    "            while b_begin < args['num_cand']:\n",
    "                b_cand_r = cand_r_list[b_begin: b_begin + args['num_cand']]\n",
    "                b_cand_e = cand_e_list[b_begin: b_begin + args['num_cand']]\n",
    "                b_begin += args['num_cand']\n",
    "\n",
    "                embed_cand_r = self.kge_model.relation_embedding[b_cand_r]\n",
    "                embed_cand_e = self.kge_model.entity_embedding[b_cand_e]\n",
    "                s1, s2 = None, None\n",
    "                if mode == \"head-batch\":\n",
    "                    s1 = self.kge_model.score_embedding(perturbed_embed_h, embed_cand_r, embed_cand_e, mode=mode)\n",
    "                    s2 = self.kge_model.score_embedding(embed_h, embed_cand_r, embed_cand_e, mode=mode)\n",
    "                elif mode == \"tail-batch\":\n",
    "                    s1 = self.kge_model.score_embedding(embed_cand_e, embed_cand_r, perturbed_embed_t, mode=mode)\n",
    "                    s2 = self.kge_model.score_embedding(embed_cand_e, embed_cand_r, embed_t, mode=mode)\n",
    "                score = self.score_func(s1, s2)\n",
    "                score = score.detach().cpu().numpy().tolist()\n",
    "                cand_scores += score\n",
    "        cand_scores = np.array(cand_scores)\n",
    "        idx = np.argmax(cand_scores)\n",
    "        score = cand_scores[idx]\n",
    "        if mode == \"head-batch\":\n",
    "            return (h, cand_r_list[idx], cand_e_list[idx]), score.item()\n",
    "        return (cand_e_list[idx], cand_r_list[idx], t), score.item()\n",
    "\n",
    "    def get_noise_triples(self):\n",
    "        noise_triples, args = self.noise_triples, self.args\n",
    "        args['num_cand'] = np.math.ceil((args['nentity']*args['nrelation'])*args['corruption_factor'] / 100)\n",
    "        all_true_triples = set(self.input_data.all_true_triples)\n",
    "        for i in range(len(self.target_triples)):\n",
    "            sys.stdout.write(\"%d in %d\\r\" % (i, len(self.target_triples)))\n",
    "            sys.stdout.flush()\n",
    "            target_triple = self.target_triples[i]\n",
    "            noise_triple_h, score_h = self.get_noise_for_head(target_triple, mode=\"head-batch\")\n",
    "            noise_triple_t, score_t = self.get_noise_for_head(target_triple, mode=\"tail-batch\")\n",
    "            if score_h > score_t:\n",
    "                noise_triples.add(noise_triple_h)\n",
    "                self.add_true_triple(noise_triple_h)\n",
    "            else:\n",
    "                noise_triples.add(noise_triple_t)\n",
    "                self.add_true_triple(noise_triple_t)\n",
    "        return list(noise_triples)\n",
    "\n",
    "class CentralDiffAddition(DirectAddition):\n",
    "    def __init__(self, args):\n",
    "        super(CentralDiffAddition, self).__init__(args)\n",
    "        self.name = \"central_diff\"\n",
    "        self.args['epsilon'] = self.args['learning_rate']\n",
    "\n",
    "    def get_noise_for_head(self, test_triple, mode=\"head-batch\"):\n",
    "        args = self.args\n",
    "        h, r, t = test_triple\n",
    "        true_cand = self.true_rel_tail[h] if mode == \"head-batch\" else self.true_rel_head[t]\n",
    "        cand_r_list = random.choices(self.all_relations, k=args['num_cand'])\n",
    "        cand_e_list = random.choices(self.all_entities, k=args['num_cand'])\n",
    "        cand_r_e_list = list(set(zip(cand_r_list, cand_e_list)).difference(true_cand))\n",
    "        cand_r_list, cand_e_list = zip(*cand_r_e_list)\n",
    "        cand_r_list, cand_e_list = list(cand_r_list), list(cand_e_list)\n",
    "        args['num_cand'] = len(cand_r_list)\n",
    "\n",
    "        embed_h = self.kge_model.entity_embedding[h]\n",
    "        embed_r = self.kge_model.relation_embedding[r]\n",
    "        embed_t = self.kge_model.entity_embedding[t]\n",
    "        score = self.kge_model.score_embedding(embed_h, embed_r, embed_t)\n",
    "        perturbed_embed_e, enforced_embed_e = None, None\n",
    "        ########## begin difference ############\n",
    "        if mode == \"head-batch\":\n",
    "            embed_h_grad = autograd.grad(score, embed_h)[0]\n",
    "            perturbed_embed_e = embed_h - args['epsilon'] * embed_h_grad\n",
    "            enforced_embed_e = embed_h + args['epsilon'] * embed_h_grad\n",
    "        elif mode == \"tail-batch\":\n",
    "            embed_t_grad = autograd.grad(score, embed_t)[0]\n",
    "            perturbed_embed_e = embed_t - args['epsilon'] * embed_t_grad\n",
    "            enforced_embed_e = embed_t + args['epsilon'] * embed_t_grad\n",
    "        ########## end difference ############\n",
    "\n",
    "        b_begin = 0\n",
    "        cand_scores = []\n",
    "        while b_begin < args['num_cand']:\n",
    "            b_cand_r = cand_r_list[b_begin: b_begin + args['num_cand']]\n",
    "            b_cand_e = cand_e_list[b_begin: b_begin + args['num_cand']]\n",
    "            b_begin += args['num_cand']\n",
    "\n",
    "            embed_cand_r = self.kge_model.relation_embedding[b_cand_r]\n",
    "            embed_cand_e = self.kge_model.entity_embedding[b_cand_e]\n",
    "            s1, s2 = None, None\n",
    "            ########## begin difference ############\n",
    "            if mode == \"head-batch\":\n",
    "                s1 = self.kge_model.score_embedding(perturbed_embed_e, embed_cand_r, embed_cand_e, mode=mode)\n",
    "                s2 = self.kge_model.score_embedding(enforced_embed_e, embed_cand_r, embed_cand_e, mode=mode)\n",
    "            elif mode == \"tail-batch\":\n",
    "                s1 = self.kge_model.score_embedding(embed_cand_e, embed_cand_r, perturbed_embed_e, mode=mode)\n",
    "                s2 = self.kge_model.score_embedding(embed_cand_e, embed_cand_r, enforced_embed_e, mode=mode)\n",
    "            ########## end difference ############\n",
    "            score = self.score_func(s1, s2)\n",
    "            score = score.detach().cpu().numpy().tolist()\n",
    "            cand_scores += score\n",
    "        cand_scores = np.array(cand_scores)\n",
    "        idx = np.argmax(cand_scores)\n",
    "        score = cand_scores[idx]\n",
    "        if mode == \"head-batch\":\n",
    "            return (h, cand_r_list[idx], cand_e_list[idx]), score.item()\n",
    "        return (cand_e_list[idx], cand_r_list[idx], t), score.item()\n",
    "\n",
    "class DirectRelAddition(DirectAddition):\n",
    "    def __init__(self, args):\n",
    "        super(DirectRelAddition, self).__init__(args)\n",
    "        self.score_func = lambda s1, s2: args['lambda1'] * s1 - args['lambda2'] * s2\n",
    "        self.name = \"direct_rel\"\n",
    "        self.true_head_tail = {}\n",
    "        for h, r, t in self.input_data.all_true_triples:\n",
    "            if r not in self.true_head_tail:\n",
    "                self.true_head_tail[r] = set()\n",
    "            self.true_head_tail[r].add((h, t))\n",
    "\n",
    "    def get_noise_for_head(self, test_triple, mode=\"head-batch\"):\n",
    "        if mode == \"tail-batch\":\n",
    "            return test_triple, -1e9\n",
    "        args = self.args\n",
    "        h, r, t = test_triple\n",
    "        s = time.time()\n",
    "        true_cand = self.true_head_tail[r]\n",
    "        cand_h_list = random.choices(self.all_entities, k=args['num_cand'])\n",
    "        cand_t_list = random.choices(self.all_entities, k=args['num_cand'])\n",
    "        cand_h_t_list = list(set(zip(cand_h_list, cand_t_list)).difference(true_cand))\n",
    "        cand_h_list, cand_t_list = zip(*cand_h_t_list)\n",
    "        cand_h_list, cand_t_list = list(cand_h_list), list(cand_t_list)\n",
    "        args['num_cand'] = len(cand_h_list)\n",
    "        e1 = time.time()\n",
    "\n",
    "        embed_h = self.kge_model.entity_embedding[h]\n",
    "        embed_r = self.kge_model.relation_embedding[r]\n",
    "        embed_t = self.kge_model.entity_embedding[t]\n",
    "        score = self.kge_model.score_embedding(embed_h, embed_r, embed_t)\n",
    "        embed_r_grad = autograd.grad(score, embed_r)[0]\n",
    "        perturbed_embed_r = embed_r - args['epsilon'] * embed_r_grad\n",
    "        e2 = time.time()\n",
    "\n",
    "        b_begin = 0\n",
    "        cand_scores = []\n",
    "        with torch.no_grad():\n",
    "            while b_begin < args['num_cand']:\n",
    "                b_cand_h = cand_h_list[b_begin: b_begin + args['num_cand']]\n",
    "                b_cand_t = cand_t_list[b_begin: b_begin + args['num_cand']]\n",
    "                b_begin += args['num_cand']\n",
    "\n",
    "                embed_cand_h = self.kge_model.entity_embedding[b_cand_h]\n",
    "                embed_cand_t = self.kge_model.entity_embedding[b_cand_t]\n",
    "                s1 = self.kge_model.score_embedding(embed_cand_h, perturbed_embed_r, embed_cand_t, mode=mode)\n",
    "                s2 = self.kge_model.score_embedding(embed_cand_h, embed_r, embed_cand_t, mode=mode)\n",
    "                score = self.score_func(s1, s2)\n",
    "                score = score.detach().cpu().numpy().tolist()\n",
    "                cand_scores += score\n",
    "        cand_scores = np.array(cand_scores)\n",
    "        idx = np.argmax(cand_scores)\n",
    "        score = cand_scores[idx]\n",
    "        e3 = time.time()\n",
    "        self.true_head_tail[r].add((cand_h_list[idx], cand_t_list[idx]))\n",
    "        return (cand_h_list[idx], r, cand_t_list[idx]), score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3ec6a-1d99-4171-94d6-7f1fcca44fdf",
   "metadata": {},
   "source": [
    "# 3. 添加相似度噪声样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f161cb4d-d437-4108-8f01-756c36dd1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nghbrs(target_triples, train_triples):\n",
    "    '''\n",
    "    For every triple in target_triples set,\n",
    "    return the index of neighbouring triple in train triples,\n",
    "    '''\n",
    "    triple2nghbrs = {}\n",
    "    train_triples = np.array(train_triples)\n",
    "    for h, r, t in target_triples:\n",
    "        mask = (np.isin(train_triples[:, 0], [h, t]) | np.isin(train_triples[:, 2], [h, t]))\n",
    "        mask_idx = np.where(mask)[0]\n",
    "        triple2nghbrs[(h, r, t)] = [tuple(triple) for triple in train_triples[mask_idx].tolist()]\n",
    "    return triple2nghbrs\n",
    "\n",
    "\n",
    "class InstanceAttributionCos(GlobalRandomNoiseAttacker):\n",
    "    def __init__(self, args):\n",
    "        super(InstanceAttributionCos, self).__init__(args)\n",
    "        self.entity_embedding = self.kge_model.entity_embedding.data\n",
    "        self.train_triples = np.array(self.input_data.train_triples)\n",
    "        triple2nghbrs_path = os.path.join(args['data_path'], \"triple2nghbrs.pkl\")\n",
    "        if not os.path.exists(triple2nghbrs_path):\n",
    "            with open(triple2nghbrs_path, \"wb\") as fw:\n",
    "                self.triple2nghbrs = generate_nghbrs(self.target_triples, self.train_triples)\n",
    "                pickle.dump(self.triple2nghbrs, fw)\n",
    "        else:\n",
    "            with open(triple2nghbrs_path, \"rb\") as f:\n",
    "                self.triple2nghbrs = pickle.load(f)\n",
    "        print(f\"generate_nghbrs done\")\n",
    "        self.similarity_func = lambda vec, nghbr_vec: F.cosine_similarity(vec, nghbr_vec)\n",
    "        self.name = \"is_cos\"\n",
    "\n",
    "    def get_influential_triples(self):\n",
    "        args = self.args\n",
    "        influential_triples_path = os.path.join(args['init_checkpoint'], \"%s_influential_triples.pkl\" % self.name)\n",
    "        if not args['no_store'] and os.path.exists(influential_triples_path):\n",
    "            with open(influential_triples_path, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        triple2influential_triple = {}\n",
    "        for i, (h, r, t) in enumerate(self.target_triples):\n",
    "            sys.stdout.write(\"influential:\\t%d in %d\\r\" % (i, len(self.target_triples)))\n",
    "            sys.stdout.flush()\n",
    "            sample = torch.LongTensor([h, r, t]).view(1, -1)\n",
    "            if self.args['cuda']:\n",
    "                sample = sample.cuda()\n",
    "            vec_score = self.kge_model(sample, mode=\"single\", get_vec=True).view(1, -1)\n",
    "            ngbhrs = self.triple2nghbrs[(h, r, t)]\n",
    "            if len(ngbhrs) == 0:\n",
    "                print(f\"we don't need to attack {h, r, t} in {args['data_path']}\")\n",
    "                continue\n",
    "            b_beign = 0\n",
    "            nghbr_sim = []\n",
    "            while b_beign < len(ngbhrs):\n",
    "                b_ngbhrs = ngbhrs[b_beign: b_beign+args['num_cand_batch']]\n",
    "                b_beign += args['num_cand_batch']\n",
    "                b_ngbhrs = torch.LongTensor(b_ngbhrs).view(-1, 3)\n",
    "                if self.args['cuda']:\n",
    "                    b_ngbhrs = b_ngbhrs.cuda()\n",
    "                b_ngbhrs_vec = self.kge_model(b_ngbhrs, mode=\"single\", get_vec=True).view(-1, vec_score.shape[-1])\n",
    "                b_sim = self.similarity_func(vec_score, b_ngbhrs_vec).detach().cpu().numpy().tolist()\n",
    "                nghbr_sim += b_sim\n",
    "            nghbr_sim = np.array(nghbr_sim)\n",
    "            idx = np.argmax(nghbr_sim)\n",
    "            triple2influential_triple[(h, r, t)] = ngbhrs[idx]\n",
    "        if not args['no_store']:\n",
    "            with open(influential_triples_path, \"wb\") as fw:\n",
    "                pickle.dump(triple2influential_triple, fw)\n",
    "        return triple2influential_triple\n",
    "\n",
    "    def find_least_similarity_entity(self, entity, r, e, mode):\n",
    "        train_triples = np.array(self.input_data.train_triples + list(self.noise_triples))\n",
    "        ent_embed = self.kge_model.entity_embedding[entity].view(1, -1)\n",
    "        cos_sim_ent = F.cosine_similarity(ent_embed, self.entity_embedding)\n",
    "        filter_ent = None\n",
    "        if mode == \"head-mode\":\n",
    "            filter_ent = train_triples[np.where((train_triples[:, 2] == e) & (train_triples[:, 1] == r)), 0]\n",
    "        elif mode == \"tail-mode\":\n",
    "            filter_ent = train_triples[np.where((train_triples[:, 0] == e) & (train_triples[:, 1] == r)), 2]\n",
    "        cos_sim_ent[filter_ent.squeeze()] = 1e8\n",
    "        idx = torch.argmin(cos_sim_ent).item()\n",
    "        return idx\n",
    "\n",
    "    def get_noise_triples(self):\n",
    "        noise_triples = self.noise_triples\n",
    "        influential_triples = self.get_influential_triples()\n",
    "        for i in range(len(self.target_triples)):\n",
    "            sys.stdout.write(\"%d in %d\\r\" % (i, len(self.target_triples)))\n",
    "            sys.stdout.flush()\n",
    "            h, r, t = self.target_triples[i]\n",
    "            if (h, r, t) not in influential_triples:\n",
    "                continue\n",
    "            hi, ri, ti = influential_triples[(h, r, t)]\n",
    "            if ti in [h, t]:\n",
    "                fake_head = self.find_least_similarity_entity(hi, ri, ti, mode=\"head-mode\")\n",
    "                noise_triples.add((fake_head, ri, ti))\n",
    "            elif hi in [h, t]:\n",
    "                fake_tail = self.find_least_similarity_entity(ti, ri, hi, mode=\"tail-mode\")\n",
    "                noise_triples.add((hi, ri, fake_tail))\n",
    "            else:\n",
    "                print(\"unexpected behavior\")\n",
    "\n",
    "        return list(noise_triples)\n",
    "\n",
    "\n",
    "class InstanceAttributionDot(InstanceAttributionCos):\n",
    "    def __init__(self, args):\n",
    "        super(InstanceAttributionDot, self).__init__(args)\n",
    "        self.similarity_func = lambda vec, nghbr_vec: torch.matmul(vec, nghbr_vec.t())\n",
    "        self.name = \"is_dot\"\n",
    "\n",
    "\n",
    "class InstanceAttributionL2(InstanceAttributionCos):\n",
    "    def __init__(self, args):\n",
    "        super(InstanceAttributionL2, self).__init__(args)\n",
    "        self.similarity_func = lambda vec, nghbr_vec: -torch.norm((nghbr_vec-vec), p=2, dim=-1)\n",
    "        self.name = \"is_l2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83b6b2-2862-4fec-a5a4-d21795e48968",
   "metadata": {},
   "source": [
    "# 4. 添加梯度相似度噪声样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7fa7e3e-5fb7-47bf-88be-8b5c2edaed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_zero_idx(matrix1, matrix2):\n",
    "    idx1 = set(torch.nonzero(matrix1)[:, 0].detach().cpu().numpy().tolist())\n",
    "    idx2 = set(torch.nonzero(matrix2)[:, 0].detach().cpu().numpy().tolist())\n",
    "    idx = list(idx1.intersection(idx2))\n",
    "    return idx\n",
    "\n",
    "def jacobian(y: torch.Tensor, x: torch.Tensor, need_higher_grad=False) -> torch.Tensor:\n",
    "    \"\"\" refer:https://zhuanlan.zhihu.com/p/530879775\n",
    "    基于 torch.autograd.grad 函数的更清晰明了的 API，功能是计算一个雅可比矩阵。\n",
    "\n",
    "    Args:\n",
    "        y (torch.Tensor): 函数输出向量\n",
    "        x (torch.Tensor): 函数输入向量\n",
    "        need_higher_grad (bool, optional): 是否需要计算高阶导数，如果确定不需要可以设置为 False 以节约资源. 默认为 True.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 计算好的“雅可比矩阵”。注意！输出的“雅可比矩阵”形状为 y.shape + x.shape。例如：y 是 n 个元素的张量，y.shape = [n]；x 是 m 个元素的张量，x.shape = [m]，则输出的雅可比矩阵形状为 n x m，符合常见的数学定义。\n",
    "        但是若 y 是 1 x n 的张量，y.shape = [1,n]；x 是 1 x m 的张量，x.shape = [1,m]，则输出的雅可比矩阵形状为1 x n x 1 x m，如果嫌弃多余的维度可以自行使用 torch.squeeze(Jac) 一步到位。\n",
    "        这样设计是因为考虑到 y 是 n1 x n2 的张量； 是 m1 x m2 的张量（或者形状更复杂的张量）时，输出 n1 x n2 x m1 x m2 （或对应更复杂形状）更有直观含义，方便用户知道哪一个元素对应的是哪一个偏导。\n",
    "    \"\"\"\n",
    "    (Jac,) = torch.autograd.grad(\n",
    "        outputs=(y.flatten(),),\n",
    "        inputs=(x,),\n",
    "        grad_outputs=(torch.eye(torch.numel(y)).cuda(),),\n",
    "        retain_graph=True,\n",
    "        create_graph=need_higher_grad,\n",
    "        allow_unused=True,\n",
    "        is_grads_batched=True\n",
    "    )\n",
    "    if Jac is None:\n",
    "        Jac = torch.zeros(size=(y.shape + x.shape))\n",
    "    else:\n",
    "        Jac.reshape(shape=(y.shape + x.shape))\n",
    "    return Jac\n",
    "\n",
    "\n",
    "\n",
    "class InstanceAttributionCosGrad(InstanceAttributionCos):\n",
    "    def __init__(self, args):\n",
    "        super(InstanceAttributionCosGrad, self).__init__(args)\n",
    "        self.similarity_func = lambda grad, nghbr_grad: F.cosine_similarity(grad, nghbr_grad)\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        self.name = \"gs_cos\"\n",
    "\n",
    "        named_parameters = list(self.kge_model.named_parameters())\n",
    "        self.param_list = []\n",
    "        for n, p in named_parameters:\n",
    "            if p.requires_grad:\n",
    "                self.param_list.append(p)\n",
    "\n",
    "    def get_loss(self, triple):\n",
    "        h, r, t = triple[:, 0], triple[:, 1], triple[:, 2]\n",
    "        embed_h = self.kge_model.entity_embedding[h]\n",
    "        embed_r = self.kge_model.relation_embedding[r]\n",
    "        embed_t = self.kge_model.entity_embedding[t]\n",
    "        loss = 0\n",
    "        pred_h_embedding = self.kge_model.predict_embedding(embed_t, embed_r, \"head-mode\")\n",
    "        pred_h = torch.mm(pred_h_embedding.squeeze(1), self.kge_model.entity_embedding.transpose(1,0))\n",
    "        pred_h = torch.sigmoid(pred_h)\n",
    "        loss += self.loss_func(pred_h, h)\n",
    "\n",
    "        pred_t_embedding = self.kge_model.predict_embedding(embed_h, embed_r, \"tail-mode\")\n",
    "        pred_t = torch.mm(pred_t_embedding.squeeze(1), self.kge_model.entity_embedding.transpose(1,0))\n",
    "        pred_t = torch.sigmoid(pred_t)\n",
    "        loss += self.loss_func(pred_t, t)\n",
    "        return loss\n",
    "\n",
    "    # given loss of two triples, calculate the score between them\n",
    "    def calc_influential_score(self, loss1, loss2):\n",
    "        batch_size = loss2.view(-1).shape[0]\n",
    "        grad_e1 = jacobian(loss1, self.param_list[0]) # size:1*E*dim\n",
    "        grad_r1 = jacobian(loss1, self.param_list[1]) # size:1*R*dim\n",
    "        # embed()\n",
    "        grad_e2 = jacobian(loss2, self.param_list[0]) # size:B*E*dim\n",
    "        grad_r2 = jacobian(loss2, self.param_list[1]) # size:B*R*dim\n",
    "        # grad_e1, grad_r1 = autograd.grad(loss1, self.param_list, retain_graph=True)\n",
    "        # grad_e2, grad_r2 = autograd.grad(sumed_loss2, batched_param_list, retain_graph=True, \\\n",
    "        #     grad_outputs=(torch.eye(torch.numel(sumed_loss2)),), is_grads_batched=True)\n",
    "        # embed()\n",
    "        grad_e1, grad_e2 = grad_e1.view(1, -1), grad_e2.view(batch_size, -1)\n",
    "        grad_r1, grad_r2 = grad_r1.view(1, -1), grad_r2.view(batch_size, -1)\n",
    "        score = self.similarity_func(grad_e1, grad_e2) + self.similarity_func(grad_r1, grad_r2)\n",
    "        score = score.detach().cpu().numpy().tolist()\n",
    "        del grad_e1\n",
    "        del grad_e2\n",
    "        del grad_r1\n",
    "        del grad_r2\n",
    "        torch.cuda.empty_cache()\n",
    "        return score\n",
    "\n",
    "    def get_influential_triples(self):\n",
    "        args = self.args\n",
    "        influential_triples_path = os.path.join(args['init_checkpoint'], \"%s_influential_triples.pkl\" % self.name)\n",
    "        if not args['no_store'] and os.path.exists(influential_triples_path):\n",
    "            with open(influential_triples_path, \"rb\") as f:\n",
    "                triple2influential_triple = pickle.load(f)\n",
    "                if (triple2influential_triple is not None and type(triple2influential_triple) == type({1:1}) and all([triple in triple2influential_triple for triple in self.target_triples])):\n",
    "                    return triple2influential_triple\n",
    "        \n",
    "        triple2influential_triple = {}\n",
    "        for i, target_triple in enumerate(self.target_triples):\n",
    "            ngbhrs = self.triple2nghbrs[target_triple]\n",
    "            target_triple = torch.LongTensor(target_triple).view(-1, 3).cuda()\n",
    "            target_loss = self.get_loss(target_triple)\n",
    "            if len(ngbhrs) == 0:\n",
    "                print(f\"we don't need to attack {target_triple} in {args['data_path']}\")\n",
    "                continue\n",
    "            nghbr_sim = []\n",
    "            b_beign = 0\n",
    "            # for i, b_ngbhrs in enumerate(ngbhrs):\n",
    "            while b_beign < len(ngbhrs):\n",
    "                b_ngbhrs = ngbhrs[b_beign: b_beign+args['num_cand_batch']]\n",
    "                b_beign += args['num_cand_batch']\n",
    "\n",
    "                t1 = time.time()\n",
    "                b_ngbhrs = torch.LongTensor(b_ngbhrs).view(-1, 3).cuda()\n",
    "                ngbhr_loss = self.get_loss(b_ngbhrs)\n",
    "                grad_sim = self.calc_influential_score(target_loss, ngbhr_loss)\n",
    "                nghbr_sim += grad_sim\n",
    "                t2 = time.time()\n",
    "                # print(f\"time used: {t2 - t1}: {b_beign}/{len(ngbhrs)}\")\n",
    "            nghbr_sim = np.array(nghbr_sim)\n",
    "            idx = np.argmax(nghbr_sim)\n",
    "            target_triple = tuple(target_triple.view(-1).detach().cpu().tolist())\n",
    "            triple2influential_triple[target_triple] = ngbhrs[idx]\n",
    "\n",
    "            sys.stdout.write(\"influential:\\t%d in %d\\r\" % (i, len(self.target_triples)))\n",
    "            sys.stdout.flush()\n",
    "        if not args['no_store']:\n",
    "            with open(influential_triples_path, \"wb\") as fw:\n",
    "                pickle.dump(triple2influential_triple, fw)\n",
    "        return triple2influential_triple\n",
    "\n",
    "\n",
    "class InstanceAttributionDotGrad(InstanceAttributionCosGrad):\n",
    "    def __init__(self, args):\n",
    "        super(InstanceAttributionDotGrad, self).__init__(args)\n",
    "        self.similarity_func = lambda grad, nghbr_grad: torch.matmul(grad, nghbr_grad.T)\n",
    "        self.name = \"gs_dot\"\n",
    "        # embed()\n",
    "\n",
    "class InstanceAttributionL2Grad(InstanceAttributionCosGrad):\n",
    "    def __init__(self, args):\n",
    "        super(InstanceAttributionL2Grad, self).__init__(args)\n",
    "        self.similarity_func = lambda grad, nghbr_grad: -torch.norm((grad-nghbr_grad), p=2, dim=-1)\n",
    "        self.name = \"gs_l2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f962d0-f002-40e1-8ed0-478012dd8225",
   "metadata": {},
   "source": [
    "# 5. 生成transE的wn18rr噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11a9a05e-f398-4239-a37e-daac985e0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "------ direct starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:7.952876567840576\n",
      "Num Noise:100\n",
      "False Negative: 0\n",
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "------ central_diff starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:8.396076917648315\n",
      "Num Noise:100\n",
      "False Negative: 0\n",
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "------ direct_rel starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:28.697504997253418\n",
      "Num Noise:100\n",
      "False Negative: 0\n"
     ]
    }
   ],
   "source": [
    "## 修改model可以改变模型\n",
    "## 修改data_path可以改变训练集\n",
    "\n",
    "args={\n",
    "    'cuda': True, \n",
    "    'fake': None, \n",
    "    'do_train': True, \n",
    "    'do_valid': False, \n",
    "    'do_test': True, \n",
    "    'evaluate_train': False, \n",
    "    'data_path': 'data/wn18rr', \n",
    "    'model': 'TransE', \n",
    "    'double_entity_embedding': False, \n",
    "    'double_relation_embedding': False, \n",
    "    'negative_sample_size': 1024, \n",
    "    'hidden_dim': 200, \n",
    "    'gamma': 6.0, \n",
    "    'negative_adversarial_sampling': True, \n",
    "    'adversarial_temperature': 0.5, \n",
    "    'batch_size': 512, \n",
    "    'regularization': 0.0, \n",
    "    'test_batch_size': 8, \n",
    "    'uni_weight': False, \n",
    "    'learning_rate': 0.0005, \n",
    "    'cpu_num': 10, \n",
    "    'init_checkpoint': './models/TransE_wn18rr_baseline', \n",
    "    'max_steps': 40000, \n",
    "    'warm_up_steps': 20000, \n",
    "    'no_save': False, \n",
    "    'save_path': 'models/TransE_wn18rr_baseline', \n",
    "    'comments': '\\n', \n",
    "    'save_checkpoint_steps': 10000, \n",
    "    'valid_steps': 10000, \n",
    "    'log_steps': 2000, \n",
    "    'classify_steps': 5000, \n",
    "    'test_log_steps': 1000, \n",
    "    'nentity': 40943, \n",
    "    'nrelation': 11, \n",
    "    'target_triples': None, \n",
    "    'identifier': None, \n",
    "    'epsilon': 1.0, \n",
    "    'lambda1': 1.0, \n",
    "    'lambda2': 1.0, \n",
    "    'corruption_factor': 10.0, \n",
    "    'num_cand_batch': 64, \n",
    "    'no_store': False\n",
    "}\n",
    "## 攻击实体\n",
    "DirectAddition(args).generate(\"direct_10\")\n",
    "## 攻击实体    \n",
    "CentralDiffAddition(args).generate(\"central_diff_10\")\n",
    "## 攻击关系\n",
    "DirectRelAddition(args).generate(\"direct_rel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2f771-f57d-4426-878a-f588c6029b22",
   "metadata": {},
   "source": [
    "# 6. 基于相似度生成噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86762867-bccf-414b-a3d6-7c58040eb331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "generate_nghbrs done\n",
      "------ is_cos starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:1.839982032775879\n",
      "Num Noise:50\n",
      "False Negative: 0\n",
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "generate_nghbrs done\n",
      "------ is_dot starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:2.161444902420044\n",
      "Num Noise:50\n",
      "False Negative: 0\n",
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "generate_nghbrs done\n",
      "------ is_l2 starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:2.0773956775665283\n",
      "Num Noise:50\n",
      "False Negative: 0\n"
     ]
    }
   ],
   "source": [
    "# 注意删除以前生成的pkl文件\n",
    "# 余弦相似度\n",
    "InstanceAttributionCos(args).generate(\"is_cos\")\n",
    "# 点积相似度\n",
    "InstanceAttributionDot(args).generate(\"is_dot\")\n",
    "# L2范数相似度\n",
    "InstanceAttributionL2(args).generate(\"is_l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf7495-efe3-44e9-a1c1-fd41b8f06d91",
   "metadata": {},
   "source": [
    "# 7. 基于梯度相似度生成噪声样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ef5cb7-705f-43fd-995f-390aea26a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "generate_nghbrs done\n",
      "------ gs_cos starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:49.068500995635986\n",
      "Num Noise:100\n",
      "False Negative: 0\n",
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "generate_nghbrs done\n",
      "------ gs_dot starts to generate noise for TransE_wn18rr_baseline ------\n",
      "influential:\t0 in 100\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100720/1541743459.py:122: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  nghbr_sim = np.array(nghbr_sim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:48.77861666679382\n",
      "Num Noise:100\n",
      "False Negative: 0\n",
      "load model from ./models/TransE_wn18rr_baseline/checkpoint\n",
      "generate_nghbrs done\n",
      "------ gs_l2 starts to generate noise for TransE_wn18rr_baseline ------\n",
      "Time taken:48.550278425216675\n",
      "Num Noise:100\n",
      "False Negative: 0\n"
     ]
    }
   ],
   "source": [
    "# 注意删除以前生成的pkl文件，如triple2nghbrs.pkl\n",
    "## 梯度余弦\n",
    "InstanceAttributionCosGrad(args).generate(\"gs_cos\")\n",
    "## 梯度点积\n",
    "InstanceAttributionDotGrad(args).generate(\"gs_dot\")\n",
    "## 梯度L2范数\n",
    "InstanceAttributionL2Grad(args).generate(\"gs_l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf30777-9c36-46a1-a406-402ec34494c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
