{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cb4355-7a43-4ffd-8b12-0fea9e0ec78c",
   "metadata": {},
   "source": [
    "# 0. 定义工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a4831-9218-42f9-b1c8-a920cb7610df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定义数据集加载类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b62692-fde3-470b-bec5-8970f67d550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import argparse\n",
    "import pickle\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from IPython import embed\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class InputData(object):\n",
    "    def __init__(self, entity2id, relation2id, train_triples, valid_triples, test_triples, all_true_triples,\n",
    "                 fake_triples=None):\n",
    "        self.entity2id = entity2id\n",
    "        self.relation2id = relation2id\n",
    "        self.train_triples = train_triples\n",
    "        self.valid_triples = valid_triples\n",
    "        self.test_triples = test_triples\n",
    "        self.all_true_triples = all_true_triples\n",
    "        self.fake_triples = fake_triples\n",
    "\n",
    "def get_input_data(args):\n",
    "    with open(os.path.join(args['data_path'], 'entities.dict')) as fin:\n",
    "        entity2id = dict()\n",
    "        for line in fin:\n",
    "            eid, entity = line.strip().split('\\t')\n",
    "            entity2id[entity] = int(eid)\n",
    "\n",
    "    with open(os.path.join(args['data_path'], 'relations.dict')) as fin:\n",
    "        relation2id = dict()\n",
    "        for line in fin:\n",
    "            rid, relation = line.strip().split('\\t')\n",
    "            relation2id[relation] = int(rid)\n",
    "\n",
    "    args['nentity'], args['nrelation'] = len(entity2id), len(relation2id)\n",
    "\n",
    "    train_triples = read_triple(os.path.join(args['data_path'], \"train.txt\"), entity2id, relation2id)\n",
    "    valid_triples = read_triple(os.path.join(args['data_path'], 'valid.txt'), entity2id, relation2id)\n",
    "    test_triples = read_triple(os.path.join(args['data_path'], 'test.txt'), entity2id, relation2id)\n",
    "    all_true_triples = train_triples + valid_triples + test_triples\n",
    "    fake_triples = []\n",
    "    if args['fake']:\n",
    "        if args['fake'] == \"empty\":\n",
    "            fake_triples = []\n",
    "        else:\n",
    "            fake_triples = pickle.load(open(os.path.join(args['save_path'], \"%s.pkl\" % args['fake']), \"rb\"))\n",
    "        train_triples += fake_triples\n",
    "        test_triples = pickle.load(open(os.path.join(args['data_path'], \"targetTriples.pkl\"), \"rb\"))\n",
    "\n",
    "    logging.info(args['comments'])\n",
    "    logging.info('Model: %s' % args['model'])\n",
    "    logging.info('Data Path: %s' % args['data_path'])\n",
    "    logging.info('#entity: %d' % args['nentity'])\n",
    "    logging.info('#relation: %d' % args['nrelation'])\n",
    "    logging.info('#train: %d\\t#valid: %d\\t#test: %d' % (len(train_triples), len(valid_triples), len(test_triples)))\n",
    "\n",
    "    return InputData(entity2id=entity2id,\n",
    "                     relation2id=relation2id,\n",
    "                     train_triples=train_triples,\n",
    "                     valid_triples=valid_triples,\n",
    "                     test_triples=test_triples,\n",
    "                     all_true_triples=all_true_triples,\n",
    "                     fake_triples=fake_triples)\n",
    "\n",
    "def log_metrics(mode, step, metrics):\n",
    "    for metric in metrics:\n",
    "        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n",
    "\n",
    "def set_logger(args, filename=\"train\"):\n",
    "    if args['save_path'] and not os.path.exists(args['save_path']):\n",
    "        os.makedirs(args['save_path'])\n",
    "\n",
    "    today = datetime.datetime.now()\n",
    "    log_file = os.path.join(args['save_path'] or args['init_checkpoint'], '%s-%d-%d.log' % (filename, today.month, today.day))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "        level=logging.INFO,\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        filename=log_file,\n",
    "        filemode='w'\n",
    "    )\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "\n",
    "def read_triple(file_path, entity2id, relation2id):\n",
    "    '''\n",
    "    Read triples and map them into ids.\n",
    "    '''\n",
    "    triples = []\n",
    "    with open(file_path) as fin:\n",
    "        for line in fin:\n",
    "            h, r, t = line.strip().split('\\t')\n",
    "            triples.append((entity2id[h], relation2id[r], entity2id[t]))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98d5f9e-d832-46f2-89a4-e9a494b5e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, triples, nentity, nrelation, negative_sample_size, mode):\n",
    "        self.triples = triples\n",
    "        self.triple_set = set(triples)\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.negative_sample_size = negative_sample_size\n",
    "        self.mode = mode\n",
    "        self.count = self.count_frequency(triples)\n",
    "        self.true_head, self.true_tail = self.get_true_head_and_tail(self.triples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "    \n",
    "    def get_negative_sample(self, positive_sample, if_reweight=True):\n",
    "        head, relation, tail = positive_sample\n",
    "\n",
    "        subsampling_weight = torch.Tensor([1])\n",
    "        if if_reweight:\n",
    "            subsampling_weight = self.count[(head, relation)] + self.count[(tail, -relation - 1)]\n",
    "            subsampling_weight = torch.sqrt(1 / torch.Tensor([subsampling_weight]))\n",
    "        else:\n",
    "            if (relation, tail) not in self.true_head:\n",
    "                self.true_head[(relation, tail)] = [head]\n",
    "            if (head, relation) not in self.true_tail:\n",
    "                self.true_tail[(head, relation)] = [tail]\n",
    "\n",
    "        negative_sample_list = []\n",
    "        negative_sample_size = 0\n",
    "\n",
    "        while negative_sample_size < self.negative_sample_size:\n",
    "            negative_sample = np.random.randint(self.nentity, size=self.negative_sample_size * 2)\n",
    "            if self.mode == 'head-batch':\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_head[(relation, tail)],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "            elif self.mode == 'tail-batch':\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_tail[(head, relation)],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError('Training batch mode %s not supported' % self.mode)\n",
    "            negative_sample = negative_sample[mask]\n",
    "            negative_sample_list.append(negative_sample)\n",
    "            negative_sample_size += negative_sample.size\n",
    "\n",
    "        negative_sample = np.concatenate(negative_sample_list)[:self.negative_sample_size]\n",
    "\n",
    "        negative_sample = torch.from_numpy(negative_sample)\n",
    "\n",
    "        positive_sample = torch.LongTensor(positive_sample)\n",
    "\n",
    "        return positive_sample, negative_sample, subsampling_weight, self.mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_negative_sample(self.triples[idx])\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        subsample_weight = torch.cat([_[2] for _ in data], dim=0)\n",
    "        mode = data[0][3]\n",
    "        return positive_sample, negative_sample, subsample_weight, mode\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_frequency(triples, start=4):\n",
    "        '''\n",
    "        Get frequency of a partial triple like (head, relation) or (relation, tail)\n",
    "        The frequency will be used for subsampling like word2vec\n",
    "        '''\n",
    "        count = {}\n",
    "        for head, relation, tail in triples:\n",
    "            if (head, relation) not in count:\n",
    "                count[(head, relation)] = start\n",
    "            else:\n",
    "                count[(head, relation)] += 1\n",
    "\n",
    "            if (tail, -relation-1) not in count:\n",
    "                count[(tail, -relation-1)] = start\n",
    "            else:\n",
    "                count[(tail, -relation-1)] += 1\n",
    "        return count\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_true_head_and_tail(triples):\n",
    "        '''\n",
    "        Build a dictionary of true triples that will\n",
    "        be used to filter these true triples for negative sampling\n",
    "        '''\n",
    "        \n",
    "        true_head = {}\n",
    "        true_tail = {}\n",
    "\n",
    "        for head, relation, tail in triples:\n",
    "            if (head, relation) not in true_tail:\n",
    "                true_tail[(head, relation)] = []\n",
    "            true_tail[(head, relation)].append(tail)\n",
    "            if (relation, tail) not in true_head:\n",
    "                true_head[(relation, tail)] = []\n",
    "            true_head[(relation, tail)].append(head)\n",
    "\n",
    "        for relation, tail in true_head:\n",
    "            true_head[(relation, tail)] = np.array(list(set(true_head[(relation, tail)])))\n",
    "        for head, relation in true_tail:\n",
    "            true_tail[(head, relation)] = np.array(list(set(true_tail[(head, relation)])))                 \n",
    "\n",
    "        return true_head, true_tail\n",
    "\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, triples, all_true_triples, nentity, nrelation, mode):\n",
    "        self.triple_set = set(all_true_triples)\n",
    "        self.triples = triples\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        head, relation, tail = self.triples[idx]\n",
    "\n",
    "        if self.mode == 'head-batch':\n",
    "            tmp = [(0, rand_head) if (rand_head, relation, tail) not in self.triple_set\n",
    "                   else (-1, head) for rand_head in range(self.nentity)]\n",
    "            tmp[head] = (0, head)\n",
    "        elif self.mode == 'tail-batch':\n",
    "            tmp = [(0, rand_tail) if (head, relation, rand_tail) not in self.triple_set\n",
    "                   else (-1, tail) for rand_tail in range(self.nentity)]\n",
    "            tmp[tail] = (0, tail)\n",
    "        else:\n",
    "            raise ValueError('negative batch mode %s not supported' % self.mode)\n",
    "            \n",
    "        tmp = torch.LongTensor(tmp)            \n",
    "        filter_bias = tmp[:, 0].float()\n",
    "        negative_sample = tmp[:, 1]\n",
    "\n",
    "        positive_sample = torch.LongTensor((head, relation, tail))\n",
    "            \n",
    "        return positive_sample, negative_sample, filter_bias, self.mode\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        filter_bias = torch.stack([_[2] for _ in data], dim=0)\n",
    "        mode = data[0][3]\n",
    "        return positive_sample, negative_sample, filter_bias, mode\n",
    "    \n",
    "class BidirectionalOneShotIterator(object):\n",
    "    def __init__(self, dataloader_head, dataloader_tail):\n",
    "        self.iterator_head = self.one_shot_iterator(dataloader_head)\n",
    "        self.iterator_tail = self.one_shot_iterator(dataloader_tail)\n",
    "        self.step = 0\n",
    "        \n",
    "    def __next__(self):\n",
    "        self.step += 1\n",
    "        if self.step % 2 == 0:\n",
    "            data = next(self.iterator_head)\n",
    "        else:\n",
    "            data = next(self.iterator_tail)\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_shot_iterator(dataloader):\n",
    "        '''\n",
    "        Transform a PyTorch Dataloader into python iterator\n",
    "        '''\n",
    "        while True:\n",
    "            for data in dataloader:\n",
    "                yield data\n",
    "\n",
    "\n",
    "def get_test_dataset_list(test_triples, all_true_triples, args):\n",
    "    test_dataloader_head = DataLoader(\n",
    "        TestDataset(\n",
    "            test_triples,\n",
    "            all_true_triples,\n",
    "            args['nentity'],\n",
    "            args['nrelation'],\n",
    "            'head-batch'\n",
    "        ),\n",
    "        batch_size=args['test_batch_size'],\n",
    "        num_workers=max(1, args['cpu_num'] // 2),\n",
    "        collate_fn=TestDataset.collate_fn\n",
    "    )\n",
    "\n",
    "    test_dataloader_tail = DataLoader(\n",
    "        TestDataset(\n",
    "            test_triples,\n",
    "            all_true_triples,\n",
    "            args['nentity'],\n",
    "            args['nrelation'],\n",
    "            'tail-batch'\n",
    "        ),\n",
    "        batch_size=args['test_batch_size'],\n",
    "        num_workers=max(1, args['cpu_num'] // 2),\n",
    "        collate_fn=TestDataset.collate_fn\n",
    "    )\n",
    "\n",
    "    test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
    "    return test_dataset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54584253-93b0-4407-a643-50132efd2481",
   "metadata": {},
   "source": [
    "# 2. 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f38b36-a383-490a-a9ce-47d67d5648f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from IPython import embed\n",
    "\n",
    "class KGEModel(nn.Module):\n",
    "    def __init__(self, model_name, nentity, nrelation, hidden_dim, gamma, \n",
    "                 double_entity_embedding=False, double_relation_embedding=False):\n",
    "        super(KGEModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epsilon = 2.0\n",
    "        \n",
    "        self.gamma = nn.Parameter(\n",
    "            torch.Tensor([gamma]), \n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        self.embedding_range = nn.Parameter(\n",
    "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]), \n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        self.entity_dim = hidden_dim*2 if double_entity_embedding else hidden_dim\n",
    "        self.relation_dim = hidden_dim*2 if double_relation_embedding else hidden_dim\n",
    "        \n",
    "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.entity_embedding, \n",
    "            a=-self.embedding_range.item(), \n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "        \n",
    "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.relation_embedding, \n",
    "            a=-self.embedding_range.item(), \n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "        \n",
    "        #Do not forget to modify this line when you add a new model in the \"forward\" function\n",
    "        if model_name not in ['TransE', 'DistMult', 'ComplEx', 'RotatE']:\n",
    "            raise ValueError('model %s not supported' % model_name)\n",
    "            \n",
    "        if model_name == 'RotatE' and (not double_entity_embedding or double_relation_embedding):\n",
    "            raise ValueError('RotatE should use --double_entity_embedding')\n",
    "\n",
    "        \n",
    "    def forward(self, sample, mode='single', get_vec=False):\n",
    "        '''\n",
    "        Forward function that calculate the score of a batch of triples.\n",
    "        In the 'single' mode, sample is a batch of triple.\n",
    "        In the 'head-batch' or 'tail-batch' mode, sample consists two part.\n",
    "        The first part is usually the positive sample.\n",
    "        And the second part is the entities in the negative samples.\n",
    "        Because negative samples and positive samples usually share two elements \n",
    "        in their triple ((head, relation) or (relation, tail)).\n",
    "        '''\n",
    "\n",
    "        if mode == 'single':\n",
    "            batch_size, negative_sample_size = sample.size(0), 1\n",
    "            \n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=sample[:,0]\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding, \n",
    "                dim=0, \n",
    "                index=sample[:,1]\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "            tail = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=sample[:,2]\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "        elif mode == 'head-batch':\n",
    "            tail_part, head_part = sample\n",
    "            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)\n",
    "            \n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=head_part.view(-1)\n",
    "            ).view(batch_size, negative_sample_size, -1)\n",
    "            \n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding, \n",
    "                dim=0, \n",
    "                index=tail_part[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "            tail = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=tail_part[:, 2]\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "        elif mode == 'tail-batch':\n",
    "            head_part, tail_part = sample\n",
    "            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)\n",
    "            \n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=head_part[:, 0]\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding,\n",
    "                dim=0,\n",
    "                index=head_part[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "            \n",
    "            tail = torch.index_select(\n",
    "                self.entity_embedding, \n",
    "                dim=0, \n",
    "                index=tail_part.view(-1)\n",
    "            ).view(batch_size, negative_sample_size, -1)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('mode %s not supported' % mode)\n",
    "            \n",
    "        model_func = {\n",
    "            'TransE': self.TransE,\n",
    "            'RotatE': self.RotatE\n",
    "        }\n",
    "        \n",
    "        if self.model_name in model_func:\n",
    "            score = model_func[self.model_name](head, relation, tail, mode, get_vec=get_vec)\n",
    "        else:\n",
    "            raise ValueError('model %s not supported' % self.model_name)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def TransE(self, head, relation, tail, mode, get_vec=False):\n",
    "        if mode == 'head-batch':\n",
    "            score = head + (relation - tail)\n",
    "        else:\n",
    "            score = (head + relation) - tail\n",
    "        if get_vec:\n",
    "            return score\n",
    "\n",
    "        score = self.gamma.item() - torch.norm(score, p=1, dim=2)\n",
    "        return score\n",
    "    \n",
    "    def TransE_predict(self, subject, relation, mode):\n",
    "        if mode == 'head-batch':\n",
    "            return subject - relation # head = tail - relation\n",
    "        else:\n",
    "            return subject + relation # tail = head + relation\n",
    "\n",
    "    def RotatE(self, head, relation, tail, mode, get_vec=False):\n",
    "        pi = 3.14159265358979323846\n",
    "        \n",
    "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
    "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
    "\n",
    "        #Make phases of relations uniformly distributed in [-pi, pi]\n",
    "\n",
    "        phase_relation = relation/(self.embedding_range.item()/pi)\n",
    "\n",
    "        re_relation = torch.cos(phase_relation)\n",
    "        im_relation = torch.sin(phase_relation)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            re_score = re_relation * re_tail + im_relation * im_tail\n",
    "            im_score = re_relation * im_tail - im_relation * re_tail\n",
    "            re_score = re_score - re_head\n",
    "            im_score = im_score - im_head\n",
    "        else:\n",
    "            re_score = re_head * re_relation - im_head * im_relation\n",
    "            im_score = re_head * im_relation + im_head * re_relation\n",
    "            re_score = re_score - re_tail\n",
    "            im_score = im_score - im_tail\n",
    "\n",
    "        score = torch.stack([re_score, im_score], dim = 0)\n",
    "        score = score.norm(dim = 0)\n",
    "        if get_vec:\n",
    "            return score\n",
    "\n",
    "        score = self.gamma.item() - score.sum(dim = 2)\n",
    "        return score\n",
    "    \n",
    "    def RotatE_predict(self, subject, relation, mode):\n",
    "        pi = 3.14159265358979323846\n",
    "        re_subject, im_subject = torch.chunk(subject, 2, dim=2)\n",
    "        phase_relation = relation/(self.embedding_range.item()/pi)\n",
    "        re_relation = torch.cos(phase_relation)\n",
    "        im_relation = torch.sin(phase_relation)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            re_head = re_relation * re_subject + im_relation * im_subject\n",
    "            im_head = re_relation * im_subject - im_relation * re_subject\n",
    "            return torch.cat([re_head, im_head], dim=2)\n",
    "        else:\n",
    "            re_tail = re_subject * re_relation - im_subject * im_relation\n",
    "            im_tail = re_subject * im_relation + im_subject * re_relation\n",
    "            return torch.cat([re_tail, im_tail], dim=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_score(model, args, positive_sample, negative_sample, mode):\n",
    "        negative_score = model((positive_sample, negative_sample), mode=mode)\n",
    "        positive_score = model(positive_sample)\n",
    "\n",
    "        if args['negative_adversarial_sampling']:\n",
    "            # In self-adversarial sampling, we do not apply back-propagation on the sampling weight\n",
    "            negative_score = (F.softmax(negative_score * args['adversarial_temperature'], dim=1).detach()\n",
    "                              * F.logsigmoid(-negative_score)).sum(dim=1)\n",
    "        else:\n",
    "            negative_score = F.logsigmoid(-negative_score).mean(dim=1)\n",
    "\n",
    "        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)\n",
    "        # embed()\n",
    "\n",
    "        return positive_score, negative_score\n",
    "\n",
    "    @staticmethod\n",
    "    def train_step(model, optimizer, train_iterator, args):\n",
    "        '''\n",
    "        A single train step. Apply back-propation and return the loss\n",
    "        '''\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)\n",
    "\n",
    "        if args['cuda']:\n",
    "            positive_sample = positive_sample.cuda()\n",
    "            negative_sample = negative_sample.cuda()\n",
    "            subsampling_weight = subsampling_weight.cuda()\n",
    "\n",
    "        positive_score, negative_score = KGEModel.compute_score(model, args, positive_sample, negative_sample, mode)\n",
    "\n",
    "        if args['uni_weight']:\n",
    "            positive_sample_loss = - positive_score.mean()\n",
    "            negative_sample_loss = - negative_score.mean()\n",
    "        else:\n",
    "            positive_sample_loss = - (subsampling_weight * positive_score).sum() / subsampling_weight.sum()\n",
    "            negative_sample_loss = - (subsampling_weight * negative_score).sum() / subsampling_weight.sum()\n",
    "\n",
    "        loss = (positive_sample_loss + negative_sample_loss) / 2\n",
    "\n",
    "        if args['regularization'] != 0.0:\n",
    "            #Use L3 regularization for ComplEx and DistMult\n",
    "            regularization = args['regularization'] * (\n",
    "                model.entity_embedding.norm(p = 3)**3 + \n",
    "                model.relation_embedding.norm(p = 3).norm(p = 3)**3\n",
    "            )\n",
    "            loss = loss + regularization\n",
    "            regularization_log = {'regularization': regularization.item()}\n",
    "        else:\n",
    "            regularization_log = {}\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        log = {\n",
    "            **regularization_log,\n",
    "            'positive_sample_loss': positive_sample_loss.item(),\n",
    "            'negative_sample_loss': negative_sample_loss.item(),\n",
    "            'loss': loss.item()\n",
    "        }\n",
    "\n",
    "        return log\n",
    "    \n",
    "    @staticmethod\n",
    "    def test_step(model, test_triples, all_true_triples, args):\n",
    "        '''\n",
    "        Evaluate the model on test or valid datasets\n",
    "        '''\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        #Otherwise use standard (filtered) MRR, MR, HITS@1, HITS@3, and HITS@10 metrics\n",
    "        #Prepare dataloader for evaluation\n",
    "        test_dataset_list = get_test_dataset_list(test_triples, all_true_triples, args)\n",
    "\n",
    "        logs = []\n",
    "\n",
    "        step = 0\n",
    "        total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
    "        triple2mode2ranking = {}\n",
    "        with torch.no_grad():\n",
    "            for test_dataset in test_dataset_list:\n",
    "                for positive_sample, negative_sample, filter_bias, mode in test_dataset:\n",
    "                    if args['cuda']:\n",
    "                        positive_sample = positive_sample.cuda()\n",
    "                        negative_sample = negative_sample.cuda()\n",
    "                        filter_bias = filter_bias.cuda()\n",
    "\n",
    "                    batch_size = positive_sample.size(0)\n",
    "\n",
    "                    score = model((positive_sample, negative_sample), mode)\n",
    "                    score += filter_bias\n",
    "\n",
    "                    #Explicitly sort all the entities to ensure that there is no test exposure bias\n",
    "                    argsort = torch.argsort(score, dim = 1, descending=True)\n",
    "\n",
    "                    if mode == 'head-batch':\n",
    "                        positive_arg = positive_sample[:, 0]\n",
    "                    elif mode == 'tail-batch':\n",
    "                        positive_arg = positive_sample[:, 2]\n",
    "                    else:\n",
    "                        raise ValueError('mode %s not supported' % mode)\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        #Notice that argsort is not ranking\n",
    "                        ranking = (argsort[i, :] == positive_arg[i]).nonzero()\n",
    "                        assert ranking.size(0) == 1\n",
    "\n",
    "                        #ranking + 1 is the true ranking used in evaluation metrics\n",
    "                        ranking = 1 + ranking.item()\n",
    "                        logs.append({\n",
    "                            'MRR': 1.0/ranking,\n",
    "                            'MR': float(ranking),\n",
    "                            'HITS@1': 1.0 if ranking <= 1 else 0.0,\n",
    "                            'HITS@3': 1.0 if ranking <= 3 else 0.0,\n",
    "                            'HITS@10': 1.0 if ranking <= 10 else 0.0,\n",
    "                        })\n",
    "                        triple = tuple(positive_sample[i].data.tolist())\n",
    "                        if triple not in triple2mode2ranking:\n",
    "                            triple2mode2ranking[triple] = {}\n",
    "                        triple2mode2ranking[triple][mode] = ranking\n",
    "\n",
    "                    if step % args['test_log_steps'] == 0:\n",
    "                        logging.info('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "        metrics = {}\n",
    "        for metric in logs[0].keys():\n",
    "            metrics[metric] = sum([log[metric] for log in logs])/len(logs)\n",
    "\n",
    "        logging.info(\"len of triple2mode2ranking: %d\", len(triple2mode2ranking))\n",
    "        level2MRR = {1: [0, 0, 0], 10: [0, 0, 0], 100: [0, 0, 0]}\n",
    "        for triple, mode2ranking in triple2mode2ranking.items():\n",
    "            rankh, rankt = mode2ranking[\"head-batch\"], mode2ranking[\"tail-batch\"]\n",
    "            for level in [1, 10, 100]:\n",
    "                if rankh <= level and rankt <= level:\n",
    "                    level2MRR[level][0] += 2\n",
    "                    level2MRR[level][1] += rankh + rankt\n",
    "                    level2MRR[level][2] += 1.0/rankh + 1.0/rankt\n",
    "                    break\n",
    "        for level in [1, 10, 100]:\n",
    "            if level2MRR[level][0] > 0:\n",
    "                metrics[\"%d_MR\" % level] = level2MRR[level][1] / level2MRR[level][0]\n",
    "                metrics[\"%d_MRR\" % level] = level2MRR[level][2] / level2MRR[level][0]\n",
    "                metrics[\"%d_NUM\" % level] = level2MRR[level][0]\n",
    "        if not args['no_save']:\n",
    "            with open(os.path.join(args['save_path'], 'triple2ranking.pkl'), \"wb\") as fw:\n",
    "                pickle.dump(triple2mode2ranking, fw)\n",
    "        return metrics\n",
    "\n",
    "    def score_embedding(self, head, relation, tail, mode=\"simple\", get_vec=False):\n",
    "        def vec2three_dim_vec(vec):\n",
    "            if len(vec.shape) == 2:\n",
    "                return vec.unsqueeze(1)\n",
    "            elif len(vec.shape) == 1:\n",
    "                return vec.unsqueeze(0).unsqueeze(0)\n",
    "            raise f\"strange vec shape {vec.shape}\"\n",
    "        head, relation, tail = vec2three_dim_vec(head), vec2three_dim_vec(relation), vec2three_dim_vec(tail)\n",
    "\n",
    "        model_func = {\n",
    "            'TransE': self.TransE,\n",
    "            'RotatE': self.RotatE,\n",
    "        }\n",
    "\n",
    "        if self.model_name in model_func:\n",
    "            score = model_func[self.model_name](head, relation, tail, mode=\"simple\", get_vec=get_vec)\n",
    "        else:\n",
    "            raise ValueError('model %s not supported' % self.model_name)\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def predict_embedding(self, subject, relation, mode=\"head-batch\"):\n",
    "        def vec2three_dim_vec(vec):\n",
    "            if len(vec.shape) == 2:\n",
    "                return vec.unsqueeze(1)\n",
    "            elif len(vec.shape) == 1:\n",
    "                return vec.unsqueeze(0).unsqueeze(0)\n",
    "            raise f\"strange vec shape {vec.shape}\"\n",
    "        subject, relation = vec2three_dim_vec(subject), vec2three_dim_vec(relation)\n",
    "\n",
    "        model_func = {\n",
    "            'TransE': self.TransE_predict,\n",
    "            'RotatE': self.RotatE_predict,\n",
    "        }\n",
    "\n",
    "        if self.model_name in model_func:\n",
    "            result = model_func[self.model_name](subject, relation, mode=mode)\n",
    "        else:\n",
    "            raise ValueError('model %s not supported' % self.model_name)\n",
    "\n",
    "        return result\n",
    "class BaseTrainer(object):\n",
    "    def __init__(self, input_data, args, kge_model):\n",
    "        self.name = None\n",
    "        self.input_data = input_data\n",
    "        self.args = args\n",
    "        self.trainingLogs = []\n",
    "\n",
    "        self.kge_model = kge_model\n",
    "        self.lr = args['learning_rate']\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.kge_model.parameters()),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        self.warm_up_steps = args['warm_up_steps'] if args['warm_up_steps'] else args['max_steps']  # adjust learning rate\n",
    "\n",
    "        self.train_dataloader_head = DataLoader(\n",
    "            TrainDataset(input_data.train_triples, args['nentity'], args['nrelation'],\n",
    "                         args['negative_sample_size'], 'head-batch'),\n",
    "            batch_size=args['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=max(1, args['cpu_num'] // 2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        self.train_dataloader_tail = DataLoader(\n",
    "            TrainDataset(input_data.train_triples, args['nentity'], args['nrelation'],\n",
    "                         args['negative_sample_size'], 'tail-batch'),\n",
    "            batch_size=args['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=max(1, args['cpu_num'] // 2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        self.train_iterator = BidirectionalOneShotIterator(self.train_dataloader_head, self.train_dataloader_tail)\n",
    "    \n",
    "    def _warm_up_decrease_lr(self, step):\n",
    "        if step >= self.warm_up_steps:\n",
    "            self.lr = self.lr / 10\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, self.kge_model.parameters()),\n",
    "                lr=self.lr\n",
    "            )\n",
    "            self.warm_up_steps = self.warm_up_steps * 3\n",
    "            logging.info('Change learning_rate to %f at step %d' % (self.lr, step))\n",
    "\n",
    "    def save_model(self, save_variable_list={}):\n",
    "        args = self.args\n",
    "        if args['no_save']:\n",
    "            return\n",
    "        argparse_dict = vars(args)\n",
    "        with open(os.path.join(args['save_path'], 'config.json'), 'w') as fjson:\n",
    "            json.dump(argparse_dict, fjson)\n",
    "\n",
    "        checkpoint = {\n",
    "            **save_variable_list,\n",
    "            'model_state_dict': self.kge_model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(args['save_path'], 'checkpoint'))\n",
    "\n",
    "    # basic train functions\n",
    "    def periodic_check(self, step):\n",
    "        args, input_data = self.args, self.input_data\n",
    "        if step % args['log_steps'] == 0:\n",
    "            metrics = {}\n",
    "            for metric in self.trainingLogs[0].keys():\n",
    "                metrics[metric] = sum([log[metric] for log in self.trainingLogs]) / len(self.trainingLogs)\n",
    "            log_metrics('Training average', step, metrics)\n",
    "            self.trainingLogs = []\n",
    "\n",
    "        self._warm_up_decrease_lr(step)\n",
    "\n",
    "        if step % args['save_checkpoint_steps'] == 0:\n",
    "            self.save_model()\n",
    "\n",
    "        if args['do_valid'] and step % args['valid_steps'] == 0:\n",
    "            logging.info('Evaluating on Valid Dataset...')\n",
    "            metrics = self.kge_model.test_step(self.kge_model, input_data.valid_triples, input_data.all_true_triples, args)\n",
    "            log_metrics('Valid', step, metrics)\n",
    "\n",
    "    def basicTrainStep(self, step):\n",
    "        log = self.kge_model.train_step(self.kge_model, self.optimizer, self.train_iterator, self.args)\n",
    "        self.trainingLogs.append(log)\n",
    "        self.periodic_check(step)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_trainer(input_data, args):\n",
    "        kge_model = KGEModel(\n",
    "            model_name=args['model'],\n",
    "            nentity=args['nentity'],\n",
    "            nrelation=args['nrelation'],\n",
    "            hidden_dim=args['hidden_dim'],\n",
    "            gamma=args['gamma'],\n",
    "            double_entity_embedding=args['double_entity_embedding'],\n",
    "            double_relation_embedding=args['double_relation_embedding']\n",
    "        )\n",
    "        if args['cuda']:\n",
    "            kge_model = kge_model.cuda()\n",
    "        trainer = BaseTrainer(input_data, args, kge_model)\n",
    "\n",
    "        logging.info('Model Parameter Configuration:')\n",
    "        for name, param in kge_model.named_parameters():\n",
    "            logging.info('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
    "\n",
    "        return trainer\n",
    "\n",
    "    def load_model(self):\n",
    "        print(f\"load model from {os.path.join(self.args['init_checkpoint'], 'checkpoint')}\")\n",
    "        checkpoint = torch.load(os.path.join(self.args['init_checkpoint'], 'checkpoint'))\n",
    "        self.kge_model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c00d20-927f-4a4f-aa15-39f009edf6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
